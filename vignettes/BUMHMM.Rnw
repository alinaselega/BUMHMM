%\VignetteIndexEntry{An Introduction to the BUM-HMM pipeline}
%\VignetteKeywords{BUM-HMM, structure probing, RNA}
%\VignettePackage{BUMHMM}
%\VignetteEngine{knitr::knitr}

\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE, width=90)
@

\title{BUM-HMM computational pipeline}

\author{Alina Selega}

\maketitle

This vignette provides an example workflow for running the Beta-Uniform Mixture
hidden Markov model (BUM-HMM) computational pipeline \cite{selega2016robust}.
BUM-HMM provides a statistical framework for computing per-nucleotide posterior
probabilities of modification from the reactivity scores obtained in an RNA
structure probing experiment such as SHAPE \cite{spitale2013rna} or ChemModSeq
\cite{hector2014snapshots}.

\section{Data format}

The pipeline requires three data sets: the coverage, the number of times that
the reverse transcriptase dropped off, and the rate of this drop-off for each
nucleotide position of the RNA molecule of interest. (The reverse transcriptase
drop-off reflects the modification of a site by the chemical probe.) The
coverage and the drop-off counts are the data obtained in an experiment and the
drop-off rate $r$ at each nucleotide is computed as the ratio between the
drop-off count $k$ and the coverage $n$:

\[r = \frac{k}{n}\]

All data sets are expected to be in the matrix format, with rows corresponding
to nucleotide positions and columns corresponding to experimental replicates
(all control replicates followed by all treatment replicates). The key strength
of BUM-HMM is accounting for the variability of the data and thus it requires
data sets available in multiple replicates. The three matrices for coverage,
drop-off counts, and drop-off rates should only hold numerical values and no
information about the chromosome or the nucleotide position. For
transcriptome-wide experiments, the data over different chromosomes should be
concatenated row-wise in a single matrix.

We provide a data set obtained in a structure probing experiment on 18S
ribosomal RNA with the DMS chemical probing agent, available in triplicates.

This data set has three matrices \texttt{covFile}, \texttt{docFile}, and
\texttt{dorFile}, representing the coverage, drop-off counts, and drop-off rates
for each of the 1800 nucleotides of the 18S molecule. The data set has 3 control
and 3 treatment experimental replicates, which are arranged column-wise with
control replicates first (columns 1-3) and treatment replicates following
(columns 4-6). The data set also has a string `sequence` with the corresponding
genetic sequence, required for correcting the sequence-dependent bias.

<<>>=
library(BUMHMM)
pos = 300
paste0("coverage = ", covFile[pos, 1], "; drop-off count = ", docFile[pos, 1],
       "; drop-off rate = ", signif(dorFile[pos, 1], 3), "; base = ",
       substr(sequence, pos, pos))
@

Thus, we see that the \Sexpr{pos}th nucleotide
\Sexpr{substr(sequence, pos, pos)} had coverage of \Sexpr{covFile[pos, 1]} in
the first control experimental replicate, of which the reverse transcriptase
stopped at that nucleotide \Sexpr{docFile[pos, 1]} times, giving it a drop-off
rate of \Sexpr{signif(dorFile[pos, 1], 3)}.

<<>>=
paste0("coverage = ", covFile[pos, 4], "; drop-off count = ", docFile[pos, 4],
       "; drop-off rate = ", signif(dorFile[pos, 4], 3), "; base = ",
       substr(sequence, pos, pos))
@

In the first treatment experimental replicate, that is, in the presence of a
chemical probe, the coverage and drop-off count at that position were higher but
the drop-off rate remained similar, \Sexpr{signif(dorFile[pos, 4], 3)}.

\section{The overview of pipeline}

The logic of structure probing experiments associates the accessibility of a
nucleotide with structural flexibility, i.e. double-stranded nucleotides or
those otherwise protected (e.g. by a protein interaction) will not be available
for an interaction with the chemical reagent. In contrast, those nucleotides
that are located in flexible regions, could be chemically modified by the
reagent and will therefore be at the positions at which the reverse
transcriptase drops off. Thus, these nucleotides are expected to have a high
drop-off rate.

To distinguish the true positives from noisy observations, BUM-HMM compares the
drop-off rates at each nucleotide position between the control experimental
replicates:

\[log \Big( \frac{r_{C_i}}{r_{C_j}} \Big) \]

If the drop-off rates $r_{C_i}$ and $r_{C_j}$ are similar in a pair of
replicates ($C_i$ and $C_j$), the above log-ratio will be close to 0, indicating
little variability. In contrast, different drop-off rates would result in a
large log-ratio (in absolute value). Computing these per-nucleotide log-ratios
for all pairs of control experimental replicates defines a
\textit{null distribution} which quantifies the amount of noise observed in
control conditions. Anything within that range could be simple noise. (Note that
due to a log transform, drop-off rates $r = 0$ are not allowed.)

We now compare the drop-off rates between all pairs of control and treatment
experimental replicates:

\[log \Big( \frac{r_{T_i}}{r_{C_j}} \Big) \]

The assumption is that a flexible nucleotide will have a much larger drop-off
rate in a treatment experiment ($T_i$) compared to control conditions ($C_j$),
and thus generate a large log-ratio for this pair of replicates. By comparing
treatment-contol log-ratios to the null distribution in control conditions, we
can find those nucleotides that demonstrate differences in drop-off rate larger
than those that can be expected by chance.

\section{Selecting pairs of nucleotides}

The first step of the pipeline selects pairs of nucleotide positions for
computing log-ratios and the positions for which the posterior probabilities of
modification will be computed. This is implemented with the function
\texttt{selectNuclPos}. The function takes the \texttt{covFile} and
\texttt{docFile} matrices, the numbers of control and treatment experimental
replicates in the data set (\texttt{Nc} and \texttt{Nt}, correspondingly), and a
user-specified coverage threshold \texttt{t}. Nucleotides with coverage $n < t$
will not be considered. Posterior probabilities are computed for nucleotides
with minimum allowed coverage in all experimental replicates and a non-zero
drop-off count in at least one treatment replicate.

In our data set, we have 3 control and 3 treatment replicates, so if we set the
minimum allowable coverage to 1, we can make the following function call:

<<>>=
Nc = 3
Nt = 3
t = 1
nuclSelection <- selectNuclPos(covFile, docFile, Nc, Nt, t)
@

The function \texttt{selectNuclPos} returns a list with three elements:

\begin{itemize}

  \item \texttt{analysedC} is a list where each element corresponds to a
  control-control replicate comparison. Each element holds indices of
  nucleotides that have coverage $n >= 1$ and a drop-off count $k > 0$ in both
  replicates of that comparison.

  \item \texttt{analysedCT} is a list where each element corresponds to a
  treatment-control replicate comparison. Again, each element holds indices of
  nucleotides that have coverage $n >= 1$ and a drop-off count $k > 0$ in both
  replicates of that comparison.

  \item \texttt{computePosteriors} is a list where each element corresponds to a
  treatment replicate. Each element holds indices of nucleotides that have
  coverage $n >= 1$ in all replicates and a drop-off count $k > 0$ in that
  experimental replicate.

\end{itemize}

<<>>=
length(nuclSelection$analysedC[[1]])
length(nuclSelection$analysedCT[[1]])
@

In this case, we select \Sexpr{length(nuclSelection$analysedC[[1]])} nucleotides
for the first control-control comparison and
\Sexpr{length(nuclSelection$analysedCT[[1]])} for the first treatment-control
comparison.

\section{Scaling the drop-off rates across replicates}

Because BUM-HMM works with data collected in multiple experimental replicates,
it is important to ensure that the drop-off rates do not differ dramatically
between different replicates. Thus, the second step of the pipeline scales the
drop-off rates of all nucleotides selected for pair-wise comparisons (in other
words, those nucleotides that will be considered in the analysis) to have a
common median value. This is implemented with a function \texttt{scaleDOR},
which requires the output of the function \texttt{selectNuclPos}
(described above) and returns an updated matrix of drop-off rates
\texttt{dorFile} where the drop-off rates of each replicate (i.e. values in each
column) are scaled so that the selected drop-off rates have the same median in
all columns:

<<>>=
## Medians of original drop-off rates in each replicate
apply(dorFile, 2, median)
dorFile <- scaleDOR(dorFile, nuclSelection, Nc, Nt)
## Medians of scaled drop-off rates in each replicate
apply(dorFile, 2, median)
@

After scaling, medians are much more similar across replicates (they are not
exactly equal when computed this way as not all nucleotides were selected for
different pair-wise comparisons.)

\section{Computing stretches of nucleotide positions}

The next step in the BUM-HMM modelling approach enforces a smoothness assumption
over the state of nucleotides: chemical modification does not randomly switch
along the chromosome, rather, continuous stretches of RNA are either flexible or
not. This is captured with a hidden Markov model (HMM) with binary latent
states, corresponding to the true state of each nucleotide: modified or
unmodified.

The observations of the HMM are the $p$-values associated with each nucleotide
arising from treatment-control comparisons. Modelling $p$-values directly and
having prior expectations about the log-ratios arising from the comparisons
involving modified nucleotides enabled us to model the emission distribution
with a Beta-Uniform mixture model. Further details can be found in
\cite{selega2016robust}.

To run HMM, we compute uninterrupted stretches of nucleotides for which the
posterior probabilities are to be computed. This is achieved with the function
\texttt{computeStretches}, which requires a sorted list of all nucleotide
positions for which posterior probabilities are required. This list can be
obtained from the third element of \texttt{nuclSelection}, the output of the
\texttt{selectNuclPos} function. This element holds indices of such nucleotides
selected from each treatment replicate; a union of those gives all nucleotides
for which the model can compute the probability of modification.

<<>>=
allNucleotides <- Reduce(union, nuclSelection$computePosteriors)
stretches <- computeStretches(sort(allNucleotides))
@

The function returns a list where each element corresponds to an uninterrupted
stretch; its first element indicates the start index and the second element -
the end index of the stretch. HMM will be run separately on each stretch.

<<>>=
head(stretches)
@

We will compute posterior probabilities for all nucleotides but one, which is at
the 1748th position.

\section{Bias correction}

Using a transcriptome-wide data set, we identified sequence and coverage as
factors that influence log-ratios in control conditions, that is, in the absence
of any reagent. We would therefore like to transform the log-ratios such that
these biases are eliminated and the performed comparisons are not confounded.

\subsection{Coverage bias}

The coverage bias is addressed by a variance stabilisation strategy, implemented
by the \texttt{stabiliseVariance} function. This function aims to find a
functional relationship between the LDRs in the null distribution and the
average coverage in the corresponding pair of control replicates. This
relationship is modelled with the assumption that the drop-off count is a
binomially distributed random variable (see \cite{selega2016robust} for details)
and is fitted to the data with a non-linear least squares technique. For
efficiency (i.e. running on transciptome-wide data sets), the mean coverage
values and LDRs are split into bins and average values across each bin are
fitted. Then, all LDRs (both for control-control and treatment-control
comparisons) are transformed accordingly, so that the dependency on the coverage
is eliminated.

The function requires the coverage and drop-off rate matrices, the positions of
nucleotides selected for pairwise comparisons, and the numbers of replicates.
The LDR null distribution is computed by using the values at the nucleotide
positions selected for control-control comparisons. Then, the parameters are
fitted as described above, the LDRs selected for all treatment-control
comparisons are computed, and both them and the null distribution are
transformed accordingly. Finally, the function returns a list with two elements:

\begin{itemize}

  \item \texttt{LDR\_C} is a matrix with transformed log-ratios for
  control-control comparisons.
  \item \texttt{LDR\_CT} is a matrix with transformed log-ratios for
  treatment-control comparisons. Both matrices have rows corresponding to
  nucleotides and columns to a pair-wise comparison.

\end{itemize}

<<>>=
varStab <- stabiliseVariance(covFile, dorFile, nuclSelection$analysedC,
                        nuclSelection$analysedCT, Nc, Nt)
LDR_C <- varStab$LDR_C
LDR_CT <- varStab$LDR_CT

hist(LDR_C, breaks = 30, main = 'Null distribution of LDRs')
@

\subsection{Sequence bias}

The sequence-dependent bias is addressed by computing different null
distributions of LDRs for different patterns of nucleotides. One could consider
trinucleotide patterns, reflecting an assumption that the previous and next
neighbours of a nucleotide could affect its accessibility; patterns of other
lengths cold also be considered. The function \texttt{nuclPerm} returns a vector
of all permutations of four nucleobases (A, T, G, and C) of length $n$:

<<>>=
nuclNum <- 3
patterns <- nuclPerm(nuclNum)
patterns
@

Considering trinucleotide patterns will result in computing
\Sexpr{length(patterns)} different null distributions of LDRs, each
corresponding to one pattern. To do this, we first need to find all occurrences
of each pattern within the sequence. This is implemented with the function
\texttt{findPatternPos}, which takes the list of patterns, a string containing
the sequence, and a parameter indicating whether we are dealing with sense (+)
or anti-sense (-) DNA strand.

<<>>=
nuclPosition <- findPatternPos(patterns, sequence, '+')
patterns[[1]]
head(nuclPosition[[1]])
@

In the above example, pattern \Sexpr{patterns[[1]]} appears on the positions
indicated above. For each pattern, we would like to collect the LDRs at the
middle nucleotide to compute a pattern-specific null distribution. These middle
positions can be computed as follows:

<<>>=
nuclPosition <- lapply(nuclPosition, function(x)
                x[, 1] + (ceiling(nuclNum / 2) - 1))
head(nuclPosition[[1]])
@

\section{Computing posterior probabilities with HMM}

We are now ready to run the HMM and compute posterior probabilities of
modification on our example data set. Due to the short length of the 18S
molecule, we will be omitting the sequence bias-correcting step (which is mostly
designed for transcriptome studies). Instead, we will use all nucleotide
positions:

<<>>=
nuclPosition = list()
nuclPosition[[1]] = 1:nchar(sequence)
## First position
nuclPosition[[1]][1]
## Last position
nuclPosition[[1]][length(nuclPosition[[1]])]
@

We are going to run the HMM on all uninterrupted stretches of nucleotides.
However, it is possible to only select stretches of interest, e.g. those
overlapping with particular genes.

The function \texttt{computeProbs} computes the posterior probabilities of
modification for all nucleotides, selected in the list
\texttt{nuclSelection\$computePosteriors} returned by the \texttt{selectNuclPos}
function. It requires matrices with transformed LDRs \texttt{LDR\_C} and
\texttt{LDR\_CT}, the numbers of replicates \texttt{Nc} and \texttt{Nt}, the
strand indicator, the list of positions addressing the sequence bias
\texttt{nuclPosition}, the lists of nucleotide positions selected for
pair-wise comparisons stored in \texttt{nuclSelection}, and the list of
stretches on which to run the HMM:

<<>>=
posteriors <- computeProbs(LDR_C, LDR_CT, Nc, Nt, '+', nuclPosition,
                           nuclSelection$analysedC, nuclSelection$analysedCT,
                           stretches)
@

The function first compares LDRs arising from treament-control comparisons with
the null distribution (of the corresponding nucleotide pattern if multiple
patterns are used) by computing $p$-values, defined as ${1 - q}$, for q being
the closest percentile to the given LDR.

This way, LDRs reflecting large differences in drop-off rates between treatment
and control (larger than those observed by chance) will be larger than most
values in the null distribution and will thus get a small $p$-value. These
$p$-values are then passed to the HMM and posterior probabilities are computed
for each selected nucleotide of being in the unmodified (first column in
\texttt{posteriors}) and the modified state (second column in
\texttt{posteriors}).

We see that the first few nucleotides have larger probabilities of
being unmodified by the chemical probe.

<<>>=
head(posteriors)
@

Because the positions of a transcript that were modified in the experiment are
the ones at which the reverse transcriptase drops off, the last positions of the
corresponding cDNA fragments that are detected and for which we consequently
increment the drop-off counts are just upstream of the modification sites. Thus,
we need to shift all of our probabilities up by one position:

<<>>=
shifted_posteriors <- matrix(, nrow=dim(posteriors)[1], ncol=1)
shifted_posteriors[1:(length(shifted_posteriors) - 1)] <-
                  posteriors[2:dim(posteriors)[1], 2]
@

We can now plot our probabilities to see the output of BUM-HMM for the structure
probing data obtained with DMS probe for the yeast ribosomal RNA 18S.

<<>>=
plot(shifted_posteriors, xlab = 'Nucleotide position',
     ylab = 'Probability of modification',
     main = 'BUM-HMM output for 18S DMS data set')
@

We see that most nucleotides are predicted to be in the unmodified state (and
thus could either be double-stranded or protected by a protein interaction).
However, the model also identified modified regions, which should in theory
correspond to accessible parts of the molecule.

We also provide an option to optimise the shape parameters of the Beta
distribution, which defines the emission model of the HMM for the modified
state, with the EM algorithm. To do this, the \texttt{computeProbs} function
should be called with the last parameter \texttt{optimise} set to the desired
tolerance. Once the previous and the current estimates of the parameters are
within this tolerance, the EM algorithms stops (unless it already reached the
maximum number of iterations before that). Further details can be found in
\cite{selega2016robust}.

<<eval=FALSE>>=
posteriors <- computeProbs(LDR_C, LDR_CT, Nc, Nt, '+', nuclPosition,
                           nuclSelection$analysedC, nuclSelection$analysedCT,
                           stretches, 0.001)
@

Normally, the last parameter is set to a default value of \texttt{NULL}. We
discovered during our experiments that this optimisation appeared vulnerable to
local minima. Thus, the default version of the BUM-HMM pipeline does not
currently use it.

\bibliographystyle{ieeetr}
\bibliography{vignette}

\end{document}
